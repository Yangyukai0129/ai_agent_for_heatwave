import json
from config.settings import client, MODEL
from tools.schema import TOOLS, TOOL_MAPPING

def run_agent(system_prompt, user_question, transactions):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_question}
    ]

    # ğŸ”¹ ç¬¬ä¸€éšæ®µï¼šLLM æ±ºç­–éšæ®µï¼ˆAgentï¼‰
    # ç¬¬ä¸€æ¬¡å‘¼å« LLM
    response_1_raw = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        tools=TOOLS
    )

    if not response_1_raw or not hasattr(response_1_raw, "choices"):
        print("âš ï¸ LLM å›å‚³ None æˆ–æ ¼å¼ä¸æ­£ç¢º")
        return

    assistant_msg = response_1_raw.choices[0].message
    assistant_dict = {
        "role": "assistant",
        "content": assistant_msg.content,
        "tool_calls": getattr(assistant_msg, "tool_calls", [])
    }
    messages.append(assistant_dict)
    tool_calls = assistant_dict["tool_calls"] or []

    if not tool_calls:
        print("LLM æ²’æœ‰å‘¼å«å·¥å…·")
        return

    print(f"LLM æœ‰å‘¼å« {len(tool_calls)} å€‹å·¥å…·")

    # ğŸ”¹ ç¬¬äºŒéšæ®µï¼šå·¥å…·åŸ·è¡Œéšæ®µï¼ˆAgent Coreï¼‰
    # åŸ·è¡Œå·¥å…·
    for idx, tool_call in enumerate(tool_calls, 1):
        tool_name = tool_call.function.name
        tool_args = {}
        tool_content_summary = ""

        try:
            try:
                tool_args = json.loads(tool_call.function.arguments)
            except Exception:
                import ast
                tool_args = ast.literal_eval(tool_call.function.arguments)

            if tool_name == "fp_growth_tool":
                tool_args["transactions"] = transactions
                tool_args["min_support"] = 0.06
                tool_args["min_confidence"] = 0.5

            tool_response = TOOL_MAPPING[tool_name](**tool_args)

            if tool_name == "fp_growth_tool" and isinstance(tool_response, dict):
                tool_content_summary = {
                    "total_rules": tool_response["total_rules"],
                    "near_summary": tool_response["near_rules_summary"],
                    "far_summary": tool_response["far_rules_summary"],
                    "distance_distribution": tool_response.get("distance_distribution"),
                    "top_rules_sample": tool_response.get("top_rules_sample")
                }
            else:
                tool_content_summary = f"å·¥å…· {tool_name} å·²å®Œæˆã€‚çµæœç°¡è¦ï¼š{str(tool_response)[:500]}..."

        except Exception as e:
            print(f"âš ï¸ åŸ·è¡Œå·¥å…· {tool_name} ç™¼ç”ŸéŒ¯èª¤:", e)
            tool_content_summary = f"å·¥å…· {tool_name} åŸ·è¡Œå¤±æ•—: {str(e)}"

        messages.append({
            "role": "tool",
            "tool_call_id": str(tool_call.id),
            "content": tool_content_summary
        })

    # ğŸ”¹ ç¬¬ä¸‰éšæ®µï¼šLLM å›æ‡‰éšæ®µï¼ˆLLMï¼‰
    # ç¬¬äºŒæ¬¡å‘¼å« LLM
    safe_messages = []
    for m in messages:
        if not isinstance(m, dict):
            m = m.model_dump() if hasattr(m, "model_dump") else m.__dict__
        item = {"role": m.get("role","user"), "content": str(m.get("content",""))}
        if m.get("role")=="assistant" and "tool_calls" in m and m["tool_calls"]:
            item["tool_calls"] = m["tool_calls"]
        if m.get("role")=="tool" and "tool_call_id" in m:
            item["tool_call_id"] = m["tool_call_id"]
        safe_messages.append(item)

    summary_prompt = f"""
        ä»¥ä¸‹æ˜¯ FP-Growth çµæœæ‘˜è¦ï¼š

        ç¸½è¦å‰‡æ•¸: {tool_content_summary.get("total_rules", "N/A")}

        è¿‘è·é›¢è¦å‰‡çµ±è¨ˆ (distance â‰¤ 2500 km):
        {json.dumps(tool_content_summary.get("near_summary", {}), ensure_ascii=False, indent=2)}

        é è·é›¢è¦å‰‡çµ±è¨ˆ (distance > 2500 km):
        {json.dumps(tool_content_summary.get("far_summary", {}), ensure_ascii=False, indent=2)}

        è·é›¢åˆ†å¸ƒçµ±è¨ˆ:
        {json.dumps(tool_content_summary.get("distance_distribution", {}), ensure_ascii=False, indent=2)}

        Top {len(tool_content_summary.get("top_rules_sample", []))} è¦å‰‡æ¨£æœ¬:
        {json.dumps(tool_content_summary.get("top_rules_sample", []), ensure_ascii=False, indent=2)}

        è«‹æ ¹æ“šæ‘˜è¦ç”Ÿæˆå ±å‘Šï¼ŒåŒ…å«ï¼š

        ç¸½è¦å‰‡æ•¸èˆ‡å¹³å‡å¼·åº¦ (support / confidence / lift)

        è¿‘è·é›¢èˆ‡é è·é›¢è¦å‰‡çš„ç‰¹æ€§å·®ç•°ï¼Œä¸¦èªªæ˜è·é›¢çµ±è¨ˆ (min / max / mean)

        å¯èƒ½ä»£è¡¨çš„ç†±æµªäº‹ä»¶å‚³æ’­ç‰¹å¾µï¼Œä¸¦è¨è«–è¿‘è·é›¢ vs é è·é›¢è¦å‰‡çš„æ„ç¾©
        """
    response_2_raw = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯æ°£å€™è³‡æ–™åˆ†æåŠ©ç†ï¼Œè«‹æ ¹æ“šæ‘˜è¦ç”Ÿæˆå ±å‘Š"},
            {"role": "user", "content": summary_prompt}
        ]
    )

    if not response_2_raw or not hasattr(response_2_raw, "choices"):
        print("âš ï¸ ç¬¬äºŒæ¬¡ LLM å›å‚³ None æˆ–æ ¼å¼ä¸æ­£ç¢º")
        return

    final_answer = response_2_raw.choices[0].message.content
    print("\n=== LLM æœ€çµ‚å›ç­” ===\n")
    print(final_answer)
