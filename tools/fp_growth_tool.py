# from mlxtend.frequent_patterns import fpgrowth, association_rules
# import pandas as pd
# from mlxtend.preprocessing import TransactionEncoder

# def fp_growth_tool(transactions, min_support=0.06, min_confidence=0.5):
#     print(f"ğŸš€ FP-Growth æ”¶åˆ° {len(transactions)} ç­†äº¤æ˜“")

#     # -----------------------------
#     # 1ï¸âƒ£ One-hot ç·¨ç¢¼
#     # -----------------------------
#     te = TransactionEncoder()
#     te_ary = te.fit(transactions).transform(transactions)
#     df_te = pd.DataFrame(te_ary, columns=te.columns_)

#     print(f"âœ… One-hot å®Œæˆï¼Œæ¬„ä½æ•¸: {len(df_te.columns)}")

#     # -----------------------------
#     # 2ï¸âƒ£ FP-Growth é »ç¹é …ç›®é›†
#     # -----------------------------
#     freq_itemsets = fpgrowth(df_te, min_support=min_support, use_colnames=True)
#     print(f"âœ… æ‰¾åˆ° {len(freq_itemsets)} å€‹é »ç¹é …ç›®é›†")

#     if freq_itemsets.empty:
#         print("âš ï¸ ç„¡é »ç¹é …ç›®é›†ï¼Œè¿”å›ç©ºçµæœ")
#         return []

#     # -----------------------------
#     # 3ï¸âƒ£ ç”Ÿæˆé—œè¯è¦å‰‡
#     # -----------------------------
#     rules_df = association_rules(freq_itemsets, metric="confidence", min_threshold=min_confidence)
#     print(f"âœ… ç”¢ç”Ÿ {len(rules_df)} æ¢é—œè¯è¦å‰‡")

#     if rules_df.empty:
#         return []

#     rules = []
#     for _, row in rules_df.iterrows():
#         ante = list(row['antecedents'])
#         conse = list(row['consequents'])
#         rules.append({
#             "ante": ante if len(ante) > 1 else ante[0],
#             "conse": conse if len(conse) > 1 else conse[0],
#             "support": round(row['support'], 3),
#             "confidence": round(row['confidence'], 3),
#             "lift": round(row['lift'], 3)
#         })

#     print(f"ğŸ“¦ æœ€çµ‚è¼¸å‡º {len(rules)} æ¢è¦å‰‡")
#     return rules

from mlxtend.frequent_patterns import fpgrowth, association_rules
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
import math

def latlon_to_tuple(latlon_str):
    """æŠŠ 'lat_lon' å­—ä¸²è½‰æˆ float tuple"""
    lat, lon = map(float, latlon_str.split("_"))
    return lat, lon

def calc_distance_km(ante_str, conse_str):
    """è¨ˆç®— L2 è·é›¢ä¸¦æ›ç®—æˆå…¬é‡Œ"""
    lat1, lon1 = latlon_to_tuple(ante_str)
    lat2, lon2 = latlon_to_tuple(conse_str)
    distance = math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2) * 100
    return distance

def fp_growth_tool(transactions, min_support=0.06, min_confidence=0.5):
    print(f"ğŸš€ FP-Growth æ”¶åˆ° {len(transactions)} ç­†äº¤æ˜“")

    # -----------------------------
    # 1ï¸âƒ£ One-hot ç·¨ç¢¼
    # -----------------------------
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    df_te = pd.DataFrame(te_ary, columns=te.columns_)

    print(f"âœ… One-hot å®Œæˆï¼Œæ¬„ä½æ•¸: {len(df_te.columns)}")

    # -----------------------------
    # 2ï¸âƒ£ FP-Growth é »ç¹é …ç›®é›†
    # -----------------------------
    freq_itemsets = fpgrowth(df_te, min_support=min_support, use_colnames=True)
    print(f"âœ… æ‰¾åˆ° {len(freq_itemsets)} å€‹é »ç¹é …ç›®é›†")

    if freq_itemsets.empty:
        print("âš ï¸ ç„¡é »ç¹é …ç›®é›†ï¼Œè¿”å›ç©ºçµæœ")
        return []

    # -----------------------------
    # 3ï¸âƒ£ ç”Ÿæˆé—œè¯è¦å‰‡
    # -----------------------------
    rules_df = association_rules(freq_itemsets, metric="confidence", min_threshold=min_confidence)
    print(f"âœ… ç”¢ç”Ÿ {len(rules_df)} æ¢é—œè¯è¦å‰‡")

    if rules_df.empty:
        return []

    # -----------------------------
    # 4ï¸âƒ£ æ•´ç†è¦å‰‡ä¸¦è¨ˆç®—è·é›¢
    # -----------------------------
    rules = []
    for _, row in rules_df.iterrows():
        ante = list(row['antecedents'])
        conse = list(row['consequents'])
        
        # åªè™•ç†å–®ä¸€ç¯€é»çš„æƒ…æ³ï¼Œè‹¥å¤šå€‹ç¯€é»å¯ä»¥å–ç¬¬ä¸€å€‹æˆ–å¹³å‡ç¶“ç·¯åº¦
        ante_str = ante[0] if len(ante) == 1 else ante[0]
        conse_str = conse[0] if len(conse) == 1 else conse[0]

        distance_km = calc_distance_km(ante_str, conse_str)
        distance_class = "è¿‘è·é›¢" if distance_km <= 2500 else "é è·é›¢"

        rules.append({
            "ante": ante if len(ante) > 1 else ante[0],
            "conse": conse if len(conse) > 1 else conse[0],
            "support": round(row['support'], 3),
            "confidence": round(row['confidence'], 3),
            "lift": round(row['lift'], 3),
            "distance_km": round(distance_km, 1),
            "distance_class": distance_class
        })

        # 5ï¸âƒ£ çµ±è¨ˆæ‘˜è¦
    def summarize(rules_list):
        if not rules_list:
            return {"count":0, "support_mean":None, "confidence_mean":None, "lift_mean":None}
        return {
            "count": len(rules_list),
            "support_mean": round(sum(r["support"] for r in rules_list)/len(rules_list),4),
            "confidence_mean": round(sum(r["confidence"] for r in rules_list)/len(rules_list),4),
            "lift_mean": round(sum(r["lift"] for r in rules_list)/len(rules_list),4)
        }

    near_rules = [r for r in rules if r["distance_class"]=="è¿‘è·é›¢"]
    far_rules = [r for r in rules if r["distance_class"]=="é è·é›¢"]

    summary = {
        "total_rules": len(rules),
        "near_rules_summary": summarize(near_rules),
        "far_rules_summary": summarize(far_rules),
        "top_rules_sample": sorted(rules, key=lambda x: x.get("confidence",0), reverse=True)[:20]
    }

    print(f"ğŸ“¦ æœ€çµ‚è¼¸å‡º summaryï¼Œè¦å‰‡ç¸½æ•¸ {len(rules)}")
    print('è¿‘è·é›¢',summarize(near_rules))
    print('é è·é›¢',summarize(far_rules))
    return summary
